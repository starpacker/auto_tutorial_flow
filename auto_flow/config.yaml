llm:
  provider: openai
  api_key: sk-l9Bz6YoNYxaLrVPdPhdhHTvQVOElJoGfncnipg0U0C5aqLzy
  model: claude-opus-4-5-20251101
  temperature: 0.0  # For deterministic JSON outputs

prompts:
  clean_up_code: |
    You are an expert computational scientist and good at coding.
    
    You are provided with a complete, self-contained code to implement a certain computational algorithms.
    
    The code is kind of out-dated and not structured. You should clean up the code and rearrange the main workline into 4 functions:

    1. load_and_preprocess_data(): which is used to load the data and do the preprocessing 
    2. forward_operator(): which is used to construct the observation operator or the forward operator in the computational imaging problem
    3. run_inversion(): which is using the forward operator function to solve the inverse problem, mainly including the inversion/training/optimization part.
    4. evaluate_results(): which is used to evaluate the results of the whole process, containing common computational metric like RMSE, SSIM and visualization methods.

    You should organize the provided code mainly into the above 4 functions, and it is common for the provided code containing helper functions, you should gather all these helper functions and put them before the 4 functions.
    The provided code is:
    {full_code}

    And the provided code could be run by the command {command}

    The code you give will be tested with the same command, so also keep your code would be runnable under the same command.
    IMPORTANT: You MUST wrap the entire Python code in a single markdown block
    starting with ```python and ending with ```. Do not include any text outside the code block!.

  gen_data_script: |
    You are a QA Engineer. I have a working reference code that runs with a specific command.
    I need you to generate a "Data Capture Script" named `gen_std_data.py` based on the provided reference code.

    Goal:
    Run the exact same logic as the reference code, BUT wrap the following 4 functions to save their Inputs (args, kwargs) and Outputs (return value) to disk using `numpy.savez`.

    Target Functions:
    1. load_and_preprocess_data
    2. forward_operator
    3. run_inversion
    4. evaluate_results

    Requirements:
    1. The script MUST accept the EXACT SAME command line arguments as the original code, so it can be run with the provided command.
    2. Define a wrapper/decorator that captures inputs and outputs and saves them to `{output_dir}/std_data_{{function_name}}.npz`.
    3. The .npz file must contain: `args` (object array), `kwargs` (object array/dict), and `output` (object).
    4. Apply this wrapper to the 4 target functions in the code.
    5. Keep all other logic, imports, and the `if __name__ == "__main__":` block intact.

    Reference Code:
    {code}

    IMPORTANT: Return only the full python code inside ```python ... ``` blocks.

  gen_test_script: |
    You are a Senior QA Automation Engineer. Write a robust, standalone Unit Test script `test_{func_name}.py` for `{func_name}`.

    ### 1. CONTEXT
    - **Target Function**: `{func_name}` (in `agent_{func_name}.py`).
    - **Data Source**: `{data_path}` (A `.pkl` file serialized with `dill`).
    - **Environment**: GPU is available. Inputs/Outputs might be Torch Tensors or CuPy arrays on CUDA.
    - **Verification Tool**: `verification_utils.py` (located in the same directory).

    ### 2. REFERENCE CODE
    
    **A. Target Function Signature**:
    ```python
    {standard_code}
    ```
    
    **B. Data Generation Logic (How inputs/outputs were saved)**:
    ```python
    {gen_data_code}
    ```

    ### 3. ASSIGNMENT
    Write the Python code for `test_{func_name}.py`. Follow these steps strictly:

    1. **Imports**: 
       - Standard: `sys`, `os`, `dill`.
       - Target: `from agent_{func_name} import {func_name}`.
       - Verification: `from verification_utils import recursive_check`.
    
    2. **Data Loading**:
       - Load data: `data = dill.load(open(r'{data_path}', 'rb'))`.
       - Extract: `args = data['args']`, `kwargs = data['kwargs']`, `expected_output = data['output']`.
       - *Note*: Data is already in the correct format (Tensor/Array), do NOT convert to CPU manually.

    3. **Execution**:
       - Run target: `actual_output = {func_name}(*args, **kwargs)`.

    4. **Verification**:
       - Compare: `passed, msg = recursive_check(expected_output, actual_output)`.
       - **Strictness**: The test must FAIL (exit code 1) if `passed` is False.

    5. **Reporting**:
       - **On Failure**: Print `f"TEST FAILED: {{msg}}"` and call `sys.exit(1)`.
       - **On Success**: Print `"TEST PASSED"` and call `sys.exit(0)`.

    ### 4. OUTPUT FORMAT
    Return ONLY the complete Python code inside ```python ... ``` blocks. No explanations.

  gen_inverse_test_script: |
    You are a QA Automation Engineer validating an Optimization/Inversion Algorithm `{func_name}`.
    
    ### 1. OBJECTIVE
    Since this is an optimization task, strict bit-wise equality is NOT required.
    We verify performance by running the **Standard Evaluation Function** on both the Agent's output and the Standard output, then comparing the scores.

    ### 2. CONTEXT
    - **Target Function**: `{func_name}` (from `agent_{func_name}.py`).
    - **Data Source**: `{data_path}` (contains `args`, `kwargs`, and `output` as `std_output`).
    - **Environment**: GPU available.

    ### 3. REFERENCE MATERIALS
    
    **A. Target Function Signature**:
    ```python
    {standard_code}
    ```

    **B. THE REFEREE (Standard Evaluation Logic)**:
    *You must inject this logic into your test script.*
    ```python
    {eval_code}
    ```

    ### 4. INSTRUCTIONS
    Write `test_{func_name}.py` following this logic:

    1. **Imports & Setup**:
       - Import `sys`, `os`, `dill`, `numpy as np`.
       - **CRITICAL**: Analyze **Reference B**. If `evaluate_results` uses libraries like `skimage`, `pywt`, `cv2`, `torch`, or `matplotlib`, you **MUST** import them at the top of the script.
       - Import target: `from agent_{func_name} import {func_name}`.
       - **Do NOT import `evaluate_results` from files.** Define it locally by copying **Reference B** verbatim.

    2. **Data Loading**:
       - `data = dill.load(open(r'{data_path}', 'rb'))`
       - `std_output = data['output']`

    3. **Execution**:
       - `agent_output = {func_name}(*data['args'], **data['kwargs'])`

    4. **Metric Competition**:
       - Calculate: `score_agent = evaluate_results(agent_output)`
       - Calculate: `score_std = evaluate_results(std_output)`
       
       *Handling Return Values*: 
       - If `evaluate_results` returns a tuple (e.g., `(psnr, ssim)`), use the first element as the primary score.
       - If it returns a dictionary, looks for keys like 'psnr', 'score', or 'loss'.
       - If it returns `None` (only prints), this test strategy fails. Assume it returns a numeric value.

    5. **Verification Logic**:
       - Print: `print(f"Scores -> Agent: {{score_agent}}, Standard: {{score_std}}")`
       - **Pass Condition**:
         - If Metric is "Higher is Better" (e.g., PSNR): `score_agent >= score_std * 0.95`
         - If Metric is "Lower is Better" (e.g., Loss/MSE): `score_agent <= score_std * 1.05`
         - (Default to "Higher is Better" logic if unsure, or allow small absolute difference `abs(diff) < 0.5`).
       - **Reporting**:
         - Met condition -> `sys.exit(0)`
         - Failed -> `print("Performance degradation detected."); sys.exit(1)`

    ### 5. OUTPUT FORMAT
    Return ONLY the complete Python code inside ```python ... ``` blocks.
uni_test_output_dir: /home/yjh/auto_flow/run_code/output_code/
